Out-of-sample testing refers to evaluating a model's performance on data that was not used during training. The goal is to assess how well the model generalizes to new, unseen data, helping to avoid overfitting. This is done by splitting the data into training and testing sets, or using techniques like cross-validation. It ensures the model can make accurate predictions in real-world scenarios, where it encounters new data.


**In-sample performance/fit** refers to how well a model or system performs when evaluated on the data used to train or develop it. In machine learning or statistical modeling, it indicates how accurately the model fits the training data. A good in-sample performance suggests that the model has captured the underlying patterns of the data well, but it does not guarantee that the model will generalize effectively to new, unseen data (this is where out-of-sample performance comes into play). 

In essence, **in-sample fit** measures the model's ability to explain or predict the outcomes of the data it was trained on.
